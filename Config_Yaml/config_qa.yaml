# config_qa.yaml
dataset:
  name: "squad"
  train_subset_size: 500
  val_subset_size: 100
  context_column: "context"
  question_column: "question"
  answer_column: "answers"

model:
  name: "distilbert-base-uncased"
  load_in_4bit: false
  torch_dtype: "float32"
  device_map: "auto"
  target_modules: ["q_lin", "v_lin"]

tokenizer:
  max_input_length: 384      
  max_target_length: 32       
  truncation: true
  padding: "max_length"
  prefix: ""

lora:
  task_type: "QUESTION_ANS"
  r: 8
  alpha: 16
  dropout: 0.05
  bias: "none"

train:
  epochs: 2
  batch_size: 16
  learning_rate: 5e-5
  warmup_steps: 100

evaluation:
  metric: "squad"  # supports both EM and F1

save:
  output_dir: "Tuned_Models/saved_qa_ultralight"

mlflow:
  experiment_name: "Question_Answering"
  run_name: "ultralight_run"

inference:
  example_context: "The capital of France is Paris."
  example_question: "What is the capital of France?"

hub:
  upload: true
  repo_id: anp102618/qa-ultralight-lora
  token: null